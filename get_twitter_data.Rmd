---
title: "get_twitter_data"
output: pdf_document
---

```{r packages, message = FALSE}
library(readr)
library(anytime)
library(lubridate)
library(academictwitteR)
library(tidyverse)
library(jsonlite)
library(rlist)
library(rtweet)
```
##crawling the Twitter API for relevant tweets

This part contains the whole pipeline of extracting the relevant tweets for our analysis. But before we can exploit the API, some pre-work has to be done which will be described in this section.

In the following, the main directory for our created data gets set up and the four datasets we created from the NBAStats source get loaded.
```{r data_and_specify_directory}
# set main directory to save data to
mainDir <- "./data/" 

player_metadata <- read_csv(paste(mainDir,"player-metadata.csv", sep =""))
player_game_stats <- read_csv(paste(mainDir,"player-game-stats.csv", sep =""))
game_metadata <- read_csv(paste(mainDir,"game-metadata.csv", sep =""))
player_season_stats <- read_csv(paste(mainDir,"player-season-stats.csv", sep =""))

# create directories if they don't exist
if (!dir.exists(paste(mainDir, "tweets/", sep =""))) {dir.create(paste(mainDir, "tweets/", sep =""))}
if (!dir.exists(paste(mainDir, "tweets/json/", sep =""))) {dir.create(paste(mainDir, "tweets/json/", sep =""))}
```

Before we can start with the extraction of relevant tweets for our NBA players, we have to narrow down the number of players to be considered by our pipeline. The data sets above are created over 227 players. As the process of collecting tweets for such a number of individuals would be a huge overload, we decided to pick those top players, which are most relevant for us, following some criteria we set up before.
First of all we pick those players, which continuously played in the regular seasons 2016/17 - 2018/19. We didn't consider the playoffs here, as many players didn't get into the playoffs with their teams but still played a full regular season and therefore are provide enough interesting play-data for our analysis. Furthermore, we only wanted those players in our data set, which stayed at their respective team for the whole time of observation [line 42-45]. The idea behind this was to eliminate team changes as possible factors that influence the player's performance aside those we want to observe. The resulting data was stored in the "player_season" table.

As second perimeter we inspected the variable "Box Plus/Minus" (BPM) in the player_game_stats data set, which is a score-based performance indicator that "estimates a basketball player's contribution to the team when that player is on the court. [...] BPM uses a player's box score information, position, and the team's overall performance to estimate the player's contribution in points above league average per 100 possessions played."[insert reference: https://www.basketball-reference.com/about/bpm2.html]
With this estimate, we wanted to extract those players, whose performance is relatively unstable in comparison to their colleagues by computing the standard deviation of performance for each player and storing them from the highest deviation in descending order in the "player_sd" table.

Finally we merged the "player-sd" values of the players with their respective twitter-account data, including the count of followers, the count of posted statuses, the count of accounts indicated as favourites and the player's screen name.

Now the last step was to create a final set of players we wanted to consider in our analysis by merging the two data sets created into one and picking the top intersecting players.

```{r select_relevant_players} 
# players who played for the same team during the seasons 2016/17/18/19
player_team_count <- player_season_stats %>%
  filter(Season == "2016-17" |Season == "2017-18" | Season == "2018-19") %>%
  filter(SeasonType == "Regular Season") %>%
  group_by(BBRef_Player_ID) %>%
  filter(n() == 3) %>%
  summarize(m = length(unique(Team)))

player_season <- player_team_count %>%
  filter(m == 1) %>%
  select(BBRef_Player_ID) %>%
  inner_join(player_game_stats)

# sd of players using BPM as metric  
player_sd <- player_season %>%
  group_by(BBRef_Player_ID) %>%
  summarize(sd_bpm = sd(BPM,na.rm = TRUE)) %>%
  left_join(player_metadata) %>%
  filter(sd_bpm >= 8)

# combine with twitter profile data 
player_twitter_sd <- lookup_users(player_sd %>% select(Twitter) %>% pull()) %>%
  select(followers_count, statuses_count, favourites_count, screen_name) %>%
  inner_join(player_sd, by = c("screen_name" = "Twitter"))

# list of players to choose from
final_player_list <- player_team_count %>%
  filter(m == 1) %>%
  inner_join(player_season_stats) %>%
  filter(Season == "2016-17" |Season == "2017-18" | Season == "2018-19") %>%
  filter(SeasonType == "Regular Season") %>%
  group_by(BBRef_Player_ID) %>%
  summarize(game_count = sum(G)) %>%
  inner_join(player_twitter_sd) %>%
  left_join(player_metadata)

final_player_list <- final_player_list %>%
  filter(followers_count >= 1000)
```

To successfully extract the tweets relevant to a player in the following step, we first need to merge the player data with the dates and times on which they have played matches. These times also get extracted from the "player_game_stats" data set and then filtered by the seasons we want to observe (note that we don't consider the season 2016/17 as this was only used to grant that the players also stayed at their respective team the time before our observation) as well as the selection of players we want to consider.
Then a final data set "game_time" was created, including the recently merged data as well as the variables of the "game_metadata" data set.
```{r combine_player_and_game_datetimes}
# select the relevant game ids, datetimes and player names
player_game <- player_game_stats %>%
  filter(BBRef_Player_ID %in% final_player_list$BBRef_Player_ID) %>%
  filter(Season == "2017-18" | Season == "2018-19") %>%
  select(BBRef_Player_ID, BBRef_Game_ID) %>%
  left_join(final_player_list %>% select(BBRef_Player_ID, Twitter)) 

#final dataset to use in the twitter api
game_time <- player_game %>%
  left_join(game_metadata %>% select(BBRef_Game_ID, DateTime))
```

The following function was created to extract relevant tweets related to a player. We decided to not only take into account all tweets that lie in the time before a match but limit the period to consider by the following criteria:
- the extraction should start 48 hours before a player's game
- the extraction should end 45 minutes before a player's game
especially the second limitation was selected as NBA players are not allowed to look into their mobile phones 15 minutes before a match and it is very unlikely that they even have a glimpse into it 45 minutes before, as they're already in the locker rooms, preparing for the game. The first limitation was selected by the fact that each NBA team has a match every two days, so it stood to reason to only inspect tweets in the time after the last match.
```{r save_tweets_function}
# function to get the tweets for every unique game,player combination
save_tweets <- function(player, time, game_id, starttime, endtime){
    tweets <- get_all_tweets( 
    query = player,              
    is_retweet = FALSE,
    start_tweets = starttime,
    end_tweets = endtime,
    lang = "en",
    bearer_token = bearer_token)
  
tweets <- tweets %>%
    mutate(Player = player, DateTime = time, GameId = game_id)
write_json(tweets, path = paste(mainDir, "tweets/json/", player,"_", time %>% iso8601(), ".json",sep=""))
  
  
  
  
}
```


```{r bind_tweet_jsons_function}
# change the bind_tweet_json function from academictwitteR to use a different pattern
bind_tweet_jsons <- function(player, data_path = paste(mainDir, "tweets/json",sep="")) {
  if(substr(data_path, nchar(data_path), nchar(data_path)) != "/"){
    data_path <- paste0(data_path,"/")
  }
  # parse and bind
  files <-
    list.files(
      path = file.path(data_path),
      pattern = paste("^",player,sep=""),
      recursive = T,
      include.dirs = T
    )
  
  if(length(files)<1){
    stop(paste("There are no files matching the pattern ‘",player, "_‘", " in the specified directory.", sep = ""))
  }
  
  files <- paste(data_path, files, sep = "")
  
  pb = utils::txtProgressBar(min = 0,
                             max = length(files),
                             initial = 0)
  
  json.df.all <- data.frame()
  for (i in seq_along(files)) {
    filename = files[[i]]
    json.df <- jsonlite::read_json(filename, simplifyVector = TRUE)
    json.df.all <- dplyr::bind_rows(json.df.all, json.df)
    utils::setTxtProgressBar(pb, i)
  }
  cat("\n")
  return(json.df.all)
}
```

In our final step we make the actual search for relevant tweets, utilizing the functions and calls defined beforehand. The extracted tweets then get stored into a named list to have a collection of tweets for each player we've selected as relevant.
```{r save_tweets}
#game_time <- game_time %>% tail( -126)

# a/antetgi01 201711180DAL Giannis_An34 2017-11-18 21:00:00
# 	a/antetgi01 201712220MIL Giannis_An34 2017-12-22 20:00:00
#a/antetgi01 201801200PHI Giannis_An34 2018-01-20 19:30:00
# a/antetgi01 201812050MIL Giannis_An34 2018-12-05 20:00:00
#a/antetgi01 201901040MIL Giannis_An34 2019-01-04 20:30:00
#	a/antetgi01 201901210MIL Giannis_An34 2019-01-21 14:00:00
#	 a/antetgi01 201901250MIL Giannis_An34 2019-01-25 20:30:00
#	a/antetgi01 201902020WAS Giannis_An34 2019-02-02 19:00:00
#a/antetgi01 201903170MIL Giannis_An34 2019-03-17 15:30:00
#	a/antetgi01 201903260MIL Giannis_An34 2019-03-26 20:00:00
# a/antetgi01 201905210TOR Giannis_An34 2019-05-21 20:30:00
#a/antetgi01 201905230MIL Giannis_An34 2019-05-23 20:30:00
#a/antetgi01 201905250TOR Giannis_An34 2019-05-25 20:30:00
#b/bookede01 201812190BOS DevinBook 2018-12-19 19:30:00
#b/bookede01 201902060UTA DevinBook 2019-02-06 21:00:00
#b/brownja02 201805250CLE FCHWPO 2018-05-25 20:30:00

# actual call to save all tweets 
pmap(list(
  player = game_time$Twitter,
  time = game_time$DateTime,
  game_id = game_time$BBRef_Game_ID,
  starttime = (game_time$DateTime- hours(48)) %>% iso8601() %>% paste("Z", sep = ""),
  endtime = (game_time$DateTime- minutes(45)) %>% iso8601() %>% paste("Z", sep = "")
  ),
  save_tweets)

# named list to save tweets by player
map(game_time$Twitter %>% unique(), bind_tweet_jsons) %>%
  set_names(game_time$Twitter %>% unique()) %>%
  list.save(file = paste(mainDir, "/tweets/tweets_list.RData",sep=""))
```