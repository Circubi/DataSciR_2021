---
title: The Impact of NBA player-related Social Media Posts on their on-court Performance
  - An Analysis
author: "Frank Dreyer, Kolja Günther, Jannik Greif"
date: "20.05.2021"
output:
  html_document:
    df_print: paged
  pdf_document: default
subtitle: DataSciR - Project Notebook
bibliography: references.bib
csl: ieee.csl
header-includes:
- \usepackage{titling}
- \pretitle{\begin{center} \includegraphics[width=2in,height=2in]{"datascir.png"}\LARGE\\}
- \posttitle{\end{center}}
link-citations: yes
---

```{r setup, include=FALSE, eval = TRUE}
knitr::opts_chunk$set(echo = TRUE)
# get-tweets packages
library(tidyverse)
library(rtweet)
library(academictwitteR)
library(anytime)
# nba-stats packages
library(rvest)
library(naniar)
# preprocessing packages
library(stringr)
library(textclean)
# sentiment extraction packages
library(magrittr) # for %$% operator
library(sentimentr)

knitr::opts_chunk$set(
  echo = FALSE, 
  eval = FALSE, 
  message = FALSE,
  warning = FALSE
)

data_dir <- "../data"
tweets_dir <- data_dir %>% paste("tweets", sep = "/")
sentiments_dir <- data_dir %>% paste("sentiments", sep = "/")

# Twitter Api Credentials
twitter_app_name <- "DataSciR"
twitter_api_key <- "he8beW6mFPz12r8QiqtXsZbLN" # Insert API key here
twitter_api_secret_key <- "yFnZfhVitCl9fWixlImG6BRGkCX7c2HjlggHv3HgSYtnJIDokX" # Insert API key secret here
twitter_access_token <- "" # Insert access token here
twitter_access_token_secret <- "" # Insert access token secret here
twitter_bearer_token <- "AAAAAAAAAAAAAAAAAAAAAMJSQAEAAAAAxo6ezvCA34%2F%2B1gBbknootwxLdqs%3DtP8sqsQarCLEhnQXuFYXN79PxWfpG0btbSvDmBEZfePpM23Zyy" # Insert bearer-token here

twitter_token <- create_token(
  app = twitter_app_name,
  consumer_key = twitter_api_key,
  consumer_secret = twitter_api_secret_key, 
  access_token = twitter_access_token,
  access_secret = twitter_access_token_secret
)
```

\newpage

## Github Repository

The project is documented at: https://github.com/jannikgreif/DataSciR_2021


## Team Members

```{r team, eval = TRUE}
library(knitr)
library(kableExtra)
team <- data.frame(Name = c("Jannik Greif","Kolja Günther","Frank Dreyer"),
                   `Course of Studies` =c("M.Sc. Wirtschaftsinformatik","M.Sc. Data and Knowledge Engineering","M.Sc. Data and Knowledge Engineering"),
                  Mail = c("jannik.greif@st.ovgu.de","kolja.guenther@st.ovgu.de","frank.dreyer@st.ovgu.de"), check.names = FALSE)
kbl(team, booktabs = T, linesep = "") %>%
  kable_styling(position = "center")%>%
  kable_styling(latex_options = "HOLD_position")
```

## 1. Overview
The present project aims to discover a significant impact of social media posts addressed to NBA players before matches with respect to their influence on these players’ game performance. For this purpose, we consider NBA players that are highly active on Twitter and extract tweets that are addressed to them within a short period of time before matches via the Twitter API. A sentiment analysis indicates the attitude of the posts and with the resulting sentiment polarity scores we test if there is a correlation between social media posts and players’ on-court performance.

## 2. Background and Motivation
With the growing presence of social media in all areas of life, allowing people from around the world to react to current events in real-time, an increasingly controversial discussion can be noticed. Today more than ever, public figures are exposed to the reactions of millions of people, observing and commenting on every step in their life that becomes public. The resulting negative impact that extensive social media usage can have on users' behavior and mental state is subject to different scientific studies [-@kapoor_advances_2018;-@berryman_social_2018]. 
 
Sports athletes, who use social media not only to communicate with peers and fans but also to promote themselves, are no exception to these issues [-@academy_does_2008]. Among researchers in the sports field, there is a consensus that the mental state of an athlete can have a significant impact on his or her performance [-@xu_measuring_2015]. However, only little research has been conducted in order to analyze if and how social media usage of athletes directly influences their performance.
Xu and Yu [-@xu_measuring_2015] tried to capture the mood of basketball players in the NBA from the tweets they posted just before a match, using sentiment analysis, to analyze how the predicted mood influenced their performance on court. Gruettner, Vitisvorakarn and Wambsganss [-@gruttner_new_2020] used a similar approach on tennis players and additionally analyzed the relationship between the number of tweets they posted before matches and their performance within the match. Even though both contributions show that athletes with a bad predicted mood tend to perform worse on-court, they suffer from two limitations: 
 
- The number of tweets an athlete posts per day is rather limited 
- The predicted moods are not free of bias since an athlete might only post tweets how he or she wants to be seen on Twitter (also indicated in [-@gruttner_new_2020])
 
Both of these limiting factors may lead to an inaccurate prediction of the mental state of athletes. \newline
We believe that the attitude of social media posts an athlete receives from peers and fans is also a good predictor for his or her performance. Ott and Puymbroeck support this claim  [-@academy_does_2008]. In their article they list cases where athletic performance appeared to be immediately influenced by the media and conclude that the media has the potential to change the performance of an athlete in a negative as well as positive way. In this analysis we aim to assess this relationship more closely by analyzing how social media posts addressed to NBA players affect their game performance. 

## 3. Related Work

## 4. The Data
The main challenge in the preprocessing phase of our project was to create suitable data sets which reflect both of our variables we wanted to include into our analysis. For the player's performance variable we needed to create a set of datasets which cover all neccessary statistics and metadata to be able to derive all needed values. For the sentiment variable of the tweets referring to one respective player, we first needed to narrow down the selection of players whose tweets we wanted to contain and then extract all tweets that are related to this set of players. How this was done is described in the following section.

### 4.1 NBA Stats Datasets

#### 4.1.1 Introduction

To get the needed data about players, games, seasons and all relevant meta-data, we extracted statistical data from [basketball-reference.com] (https://www.basketball-reference.com),a site which provides historized basketball statistics of basketball players and teams from various US American and European leagues including the NBA. From this we created local .csv files for different metrics.

#### 4.1.2 Setup & Extracting Player Metadata
To get started, we set up our environment and for extracting the data from [basketball-reference.com](https://basketball-reference.com) we extensively used the web scraping library `rvest` in addition to the `tidyverse`.

Before extracting stats about NBA players and games, we had to check, which players even have a twitter account. Fortunately for us, [basketball-reference.com](https://basketball-reference.com) provides a list of [Twitter usernames of NBA players](https://www.basketball-reference.com/friv/twitter.html), so we loaded the account names into the player-metadata.csv, along with an unique BBRef_Player_ID, which we took over from [basketball-reference.com](https://basketball-reference.com), and the clear name of the respective players.
With this set of players we now wanted to extract further statistics.

```{r player-metadata}
url <- "https://www.basketball-reference.com/friv/twitter.html"

metadata_tbl <- read_html(url) %>% 
  html_element("table.stats_table") 

player_td = metadata_tbl %>% 
  html_elements("td[data-stat=\"player\"]") 

twitter_td = metadata_tbl %>% 
  html_elements("td[data-stat=\"twitter\"]")

BBRef_Player_IDs = player_td %>% 
  html_element("a") %>% 
  html_attr("href") %>% 
  map_chr(~ str_extract(.x, "[a-z]/[a-z]+[0-9]{2}"))

player_names = player_td %>% html_text(trim = TRUE)

twitter_names = twitter_td %>% html_text(trim = TRUE)

metadata <- tibble(
  BBRef_Player_ID = BBRef_Player_IDs,
  Player = player_names,
  Twitter = twitter_names
)

file_path <- data_dir %>% paste("player-metadata.csv", sep = "/")
metadata %>% write_csv(file_path)
```

```{r show-player-metadata, eval = TRUE}
player_metadata <- data_dir %>% 
  paste("player-metadata.csv", sep = "/") %>% 
  read_csv()

player_metadata
```


#### 4.1.3 Getting Player Season Statistics
The goal of this dataset was to create a tibble which included all the statistics of players on season-level. As a basketball season is split into a regular season (comparable to our "Bundesliga"-system) and a playoff season (comparable to a tournaments k.o.-phase) which only the best teams of one regular season can pass, [basketball-reference.com](https://basketball-reference.com) provides two separate datasets, one for each season type.
To combine those datasets and map them to each player in one table, we set up loops, that check for each season, whether a player actively participated in either the regular season and/or the playoffs, extract the statistics if the condition holds true and tags each tuple of either the regular season statistics or the playoff statistics with a respective label "Regular Season" or "Playoffs". If one player didn't participate in any of the both possibilities, we tagged him with "Did not Play".
Finally the data set contained one tuple of statistics for each player and season/seasontype he participated in, including the following metrics:
```{r basketball-reference, echo = FALSE}
library(knitr)
table <- data.frame(
  Attribute = c("Starters / Reserves","MP","FG","FGA","FG%","3P","3PA","3P%","FT","FTA","FT%","ORB","DRB","TRB","AST","STL","BLK","TOV","PF","PTS","+/-"),
  `Data Type` = c("String","Timediff","Int","Int","Float","Int","Int","Float","Int","Int","Float",rep("Int",times=10)),
  Description = c("Name of player (separated in starters and reserves)","Minutes Played","Field Goals: number made shots (excluding free throws)","Field Goal Attempts = number of shot attempts (excluding free throws)","Field Goal Percentage: fraction of field goal attempts (FG/FGA)","3-Point Field Goals: number of made 3-point shots","3-Point Field Goal Attempts: number of 3-point shot attempts","3-Point Field Goal Percentage: fraction of three point shot attempts (3P/3PA)","Free Throws: number of free throw shots ","Free Throw Attempts: number of free throw shot attempts","Free Throw Percentage: fraction of free throw attempts (FT/FTA)","Offensive Rebounds","Defensive Rebounds","Total Rebounds (ORB+TRB)","Assists","Steals","Blocks","Turnovers","Personal Fouls","Points made","Estimates the players’ contribution to the team when the player is on the court"), check.names = FALSE
)
kbl(table, booktabs = T, linesep = "") %>%
  kable_styling(position = "center") %>%
  kable_styling(latex_options = "HOLD_position") %>%
  kable_styling(latex_options = "striped")
```

```{r player-season-stats}
get_player_season_stats <- function(BBRef_Player_ID) {
  
  url <- glue::glue("https://www.basketball-reference.com/players/{BBRef_Player_ID}.html")
  html <- read_html(url)
  
  html_regular_season_tbl <- html %>% html_node("table#per_game")
  html_playoffs_tbl <- html %>% html_node("table#playoffs_per_game")
  
  season_stats <- NULL
  regular_season_stats <- NULL
  playoffs_season_stats <- NULL
  
  # If player participated in regular season
  if (!is.na(html_regular_season_tbl)) {
    regular_season_stats <- html_regular_season_tbl %>% 
      html_table(trim = TRUE, convert = FALSE) %>% 
      mutate(SeasonType = "Regular Season", .after = Season)
  }
  
  # If player participated in playoffs
  if (!is.na(html_playoffs_tbl)) {
    playoffs_season_stats <- html_playoffs_tbl %>% 
      html_table(trim = TRUE, convert = FALSE) %>% 
      mutate(SeasonType = "Playoffs")
  }
  
  # If player participated in regular season or playoffs
  if (! is.null(regular_season_stats) | ! is.null(playoffs_season_stats)){
    season_stats <- regular_season_stats %>%
      bind_rows(playoffs_season_stats) %>% 
      filter(str_detect(Season, "[0-9]{4}-[0-9]{2}")) %>%   
      mutate(BBRef_Player_ID = BBRef_Player_ID, .before = Season) %>% 
      naniar::replace_with_na_all(
        condition = ~ str_detect(.x, "Did Not Play")
      ) %>% 
      type.convert(as.is = TRUE)
  }
  
  season_stats
  
}

player_season_stats <- player_metadata$BBRef_Player_ID %>% 
  map_dfr(get_player_season_stats) %>% 
  mutate(Team = if_else(! is.na(Tm), Tm, Team)) %>% 
  select(-Tm)


file_path <- data_dir %>% paste("player-season-stats.csv")
player_season_stats %>% write_csv(file_path)
```

```{r load-player-season-stats, eval = TRUE}
player_season_stats <- data_dir %>% 
  paste("player-season-stats.csv", sep = "/") %>% 
  read_csv()
```

```{r display-player-season-stats, eval = TRUE, message = TRUE}
player_season_stats
```


#### 4.1.4 Extracting Player Game Statistics
The next step was to extract performance statistics of the NBA-players on match-granularity. With this we wanted to create our main source for the performance indicators, we wanted to exploit for our exploratory analysis.
According to [Wikipedia](https://en.wikipedia.org/wiki/Twitter) Twitter was found in 2006. Probably not many NBA players had a Twitter account during that time. In 2007 only 400,000 tweets were posted per quarter. However, the popularity of Twitter skyrocketed after its founding with over 50 million daily tweets in 2010. Let's therefore only consider players that actively played from 2010 onwards. 
Since player performance metrics like +/- become rather unreliable if a player only gets a small amount of playing time, we only consider players that on average get at least two quarters of playing time (i.e. 24 minutes). 

```{r filter-relevant-players, eval = TRUE}

min_MP <- 24
min_Season <- "2010"

relevant_players <- player_season_stats %>% 
  filter(Season >= min_Season) %>% 
  group_by(BBRef_Player_ID) %>% 
  summarise(
    AVG_MP = mean(MP, na.rm = TRUE)
  ) %>% 
  ungroup() %>% 
  filter(AVG_MP >= min_MP) 

```

```{r display-relevant-player, eval = TRUE, message = TRUE}
relevant_players
```


```{r player-game-stats}

not_played_keys <- c(
  "Did Not Play", 
  "Did Not Dress", 
  "Inactive",
  "Not With Team",
  "Player Suspended"
)


get_player_game_stats <- function(BBRef_Player_ID) {
  
  print(BBRef_Player_ID)
  
  url <- glue::glue("https://www.basketball-reference.com/players/{BBRef_Player_ID}.html")
  html <- read_html(url)
  
  player_seasons <- get_player_seasons(BBRef_Player_ID)
  BBRef_Player_ID_rep <- rep(BBRef_Player_ID, times = player_seasons %>% length())
  
  basic_gamelogs <- map2_dfr(
    BBRef_Player_ID_rep, player_seasons, ~ get_gamelogs(.x, .y, "basic")
  ) 
  
  advanced_gamelogs <- map2_dfr(
    BBRef_Player_ID_rep, player_seasons, ~ get_gamelogs(.x, .y, "advanced")
  )
  
  gamelogs <- inner_join(basic_gamelogs, advanced_gamelogs) %>% 
    type.convert()
  
  gamelogs
  
}


get_player_seasons <- function(BBRef_Player_ID) {
  
  url <- glue::glue("https://www.basketball-reference.com/players/{BBRef_Player_ID}.html")
  html <- read_html(url)
  
  seasons <- html %>% 
    html_elements("th[data-stat=\"season\"]") %>% 
    html_element("a") %>% 
    html_text() %>% 
    unique() %>% 
    na.omit()
  
  seasons
  
}


get_gamelogs <- function(BBRef_Player_ID, season, gamelog_type) {
  
  url <- glue::glue(
    "https://www.basketball-reference.com/players/{id}/gamelog{type}/{s}",
    id = BBRef_Player_ID,
    type = if_else(gamelog_type == "basic", "", paste("", gamelog_type, sep = "-")),
    s = season %>% 
      str_remove("[0-9]{4}-") %>% 
      paste("0101", sep = "") %>% 
      lubridate::ymd() %>% 
      lubridate::year()
  )
  html <- read_html(url)
  
  print(url)
  
  html_regular_season_gamelog_tbl <- html %>%  
    html_node(glue::glue("table#pgl_{gamelog_type}"))
  
  # Playoffs game logs embedded in HTML comment
  html_playoffs_gamelog_tbl <- html %>% 
    html_nodes(xpath = '//comment()') %>% 
    html_text() %>% 
    paste(collapse = '') %>% 
    read_html() %>% 
    html_node(glue::glue("table#pgl_{gamelog_type}_playoffs"))
  
  regular_season_gamelogs <- NULL
  playoffs_gamelogs <- NULL
  gamelogs <- NULL
  
  # If player participated in regular season game
  if (!is.na(html_regular_season_gamelog_tbl)) {
    regular_season_gamelogs <- html_regular_season_gamelog_tbl %>% 
      html_table(trim = TRUE, convert = FALSE, header = NA) 
    
    names(regular_season_gamelogs)[6] <- "HTm"
    names(regular_season_gamelogs)[8] <- "WL"
    
    regular_season_gamelogs <- regular_season_gamelogs %>% 
      mutate(SeasonType = "Regular Season", .before = Date)
  }
  
  # If player participated in playoffs game 
  if (!is.na(html_playoffs_gamelog_tbl)) {
    playoffs_gamelogs <- html_playoffs_gamelog_tbl %>% 
      html_table(trim = TRUE, convert = FALSE, header = NA)
    
    names(playoffs_gamelogs)[6] <- "HTm"
    names(playoffs_gamelogs)[8] <- "WL"
    
    playoffs_gamelogs <- playoffs_gamelogs %>% 
      mutate(SeasonType = "Playoffs", .before = Date)
  }
  
  # If player participated in regular season or playoffs
  if (!is_null(regular_season_gamelogs) | !is_null(playoffs_gamelogs)) {
    gamelogs <- regular_season_gamelogs %>%
      bind_rows(playoffs_gamelogs) %>% 
      filter(Date != "Date") %>%      # Filter out header rows
      mutate(Season = season, .before = SeasonType) %>% 
      mutate(BBRef_Player_ID = BBRef_Player_ID, .before = Season) %>% 
      mutate(HTm = if_else(HTm == "@", Opp, Tm)) %>% 
      naniar::replace_with_na_all(~ .x %in% not_played_keys) %>% 
      mutate(BBRef_Game_ID = map2_chr(
        Date, HTm, 
        ~ paste(lubridate::ymd(.x) %>% format("%Y%m%d"), .y, sep = "0")
      ), .after = BBRef_Player_ID) %>% 
      type.convert(as.is = TRUE)
  }
  
  gamelogs
  
}


get_game_time <- function(game_url) {
  
  read_html(game_url) %>% 
    html_element("div.scorebox_meta") %>% 
    html_text() %>% 
    str_extract("[0-9]{1,2}:[0-9]{2} [A|P]M")
    
}


subselection <- relevant_players %>% head()

player_game_stats <- relevant_players$BBRef_Player_ID %>%
  map_dfr(get_player_game_stats)

file_path <- data_dir %>% paste("player-game-stats.csv", sep = "/")
player_game_stats %>% write_csv(file_path)
```

```{r load-player-game-stats, eval = TRUE}
player_game_stats <- data_dir %>% 
  paste("player-game-stats.csv", sep = "/") %>% 
  read_csv()
```

```{r display-player-game-stats, eval = TRUE, message = TRUE}
player_game_stats
```


#### 4.1.5 Creating Game Metadata
The last data source we wanted to create, was a table of metadata for each game. For this purpose we extracted the NBA schedule and results from [basketball-reference.com](https://basketball-reference.com) for each season from 2010 to 2021 column-wise and merged these columns into one tibble. This was then stored in the game-metadata.csv.

```{r game-metadata, eval = FALSE}

get_game_metadata <- function(season) {
  
  year <- season %>% str_remove("[0-9]{4}-") %>% 
    paste("0101", sep = "") %>% 
    lubridate::ymd() %>% 
    format("%Y")
  
  url <- glue::glue("https://www.basketball-reference.com/leagues/NBA_{year}_games.html")
  html <- read_html(url)
  
  game_metadata <- html %>% 
    html_element("div.filter") %>%
    html_elements("a") %>% 
    html_attr("href") %>% 
    map_dfr(get_game_schedule)
  
  game_metadata
  
}


get_game_schedule <- function(url) {
  
  url <- glue::glue("https://www.basketball-reference.com{url}")
  html <- read_html(url)
  
  html_schedule_tbl <- html %>% html_node("table#schedule")
  
  game_dates <- html_schedule_tbl %>% 
    html_element("tbody") %>% 
    html_elements("th[data-stat=\"date_game\"]") %>% 
    html_element("a") %>% 
    html_text2() %>% 
    lubridate::mdy()
  
  game_start_times <- html_schedule_tbl %>% 
    html_element("tbody") %>% 
    html_elements("td[data-stat=\"game_start_time\"]") %>% 
    html_text2() %>% 
    paste0("m")
  
  game_home_teams <- html_schedule_tbl %>% 
    html_element("tbody") %>% 
    html_elements("td[data-stat=\"home_team_name\"]") %>% 
    html_attr("csk") %>% 
    str_sub(start = 1, end = 3)
  
  game_home_pts <- html_schedule_tbl %>% 
    html_element("tbody") %>% 
    html_elements("td[data-stat=\"home_pts\"]") %>% 
    html_text2()
  
  game_visit_teams <- html_schedule_tbl %>% 
    html_element("tbody") %>% 
    html_elements("td[data-stat=\"visitor_team_name\"]") %>% 
    html_attr("csk") %>% 
    str_sub(start = 1, end = 3)
  
  game_visit_pts <- html_schedule_tbl %>% 
    html_element("tbody") %>% 
    html_elements("td[data-stat=\"visitor_pts\"]") %>% 
    html_text2()

  schedule <- tibble(
    Date = game_dates,
    Start = game_start_times,
    HomeTm = game_home_teams,
    HomePTS = game_home_pts,
    VisitTm = game_visit_teams,
    VisitPTS = game_visit_pts
  ) %>% 
    mutate(DateTime = map2_chr(Date, Start, paste), .before = Date) %>% 
    mutate(DateTime = lubridate::ymd_hm(DateTime)) %>% 
    mutate(BBRef_Game_ID = map2_chr(
      Date, HomeTm, ~ format(.x, "%Y%m%d") %>% paste(.y, sep = "0")
    ), .before = DateTime)
  
}

game_metadata <- player_season_stats %>% 
  filter(Season >= min_Season) %>% 
  pull(Season) %>% 
  unique() %>% 
  map_dfr(get_game_metadata)

file_path <- data_dir %>% paste("game-metadata.csv", sep = "/")
game_metadata %>% write_csv(file_path)

```

```{r load-game-metadata, eval = TRUE}
game_metadata <- data_dir %>% 
  paste("game-metadata.csv", sep = "/") %>% 
  read_csv()
```
As you can see in the table below, the metadata we extracted contains for each match the date and starting time, the home team with their respective points and the visitor team with their respective points.

```{r display-game-metadata, eval = TRUE, message = TRUE}
game_metadata
```


### 4.2 Getting the Twitter Data
The Twitter-Dataset contains all Tweets related to the players we want to inspect in our analysis for their on-court performance and this part contains the whole pipeline of extracting these relevant tweets. But before we can exploit the API, some pre-work has to be done.
First, we set up the main directory for our data to be created and the four datasets we previously created from the NBAStats source get loaded. In the next step and before we could start with the extraction of relevant tweets for our NBA players, we had to narrow down the number of players to be considered by our pipeline. The data sets above were created over 227 players. As the process of collecting tweets for such a number of individuals would be a huge overload, we decided to pick those top players, which are most relevant for us, following some criteria we set up in the following.

```{r read_nba_data, eval = TRUE}
player_metadata <- paste(data_dir,"player-metadata.csv", sep ="/") %>% read_csv()
player_game_stats <- paste(data_dir,"player-game-stats.csv", sep ="/") %>% read_csv()
game_metadata <- paste(data_dir,"game-metadata.csv", sep ="/") %>% read_csv()
player_season_stats <- paste(data_dir,"player-season-stats.csv", sep ="/") %>% read_csv()
```

#### 4.2.1 Players who played actively for the same team
First of all we picked those players, which continuously played in the regular seasons 2016/17 - 2018/19. We didn't consider the playoffs here, as many players didn't get into the playoffs with their teams but still played a full regular season and therefore are provide enough interesting play-data for our analysis. Furthermore, we only wanted those players in our data set, which stayed at their respective team for the whole time of observation. The idea behind this was to eliminate team switches as possible factors that influence the player's performance aside those we want to observe. Additionally we wanted only those players who had on-court time in at least 80% of the games during their regular season.

#### 4.2.2 Players whose BPM varied by a standard deviation of at least 8
As third parameter we inspected the variable "Box Plus/Minus" (BPM) in the player_game_stats data set, which is a score-based performance indicator that "estimates a basketball player's contribution to the team when that player is on the court. [...] BPM uses a player's box score information, position, and the team's overall performance to estimate the player's contribution in points above league average per 100 possessions played."[insert reference: https://www.basketball-reference.com/about/bpm2.html]
With this estimate, we wanted to extract those players, whose performance is relatively unstable in comparison to their colleagues by computing the standard deviation of performance for each player and storing them from the highest deviation in descending order. on this data set we applied a cutoff value to get only those players, whose standard deviation was higher or equal to 8.

#### 4.2.3 Players with at least 1.000 followers
The last parameter we wanted to include into our selection concerned about the players who have a minimal follower count of 1.000 users on Twitter. For this we joined our data with the twitter metadata we extracted along with the tweets themselves and addressed the variable 'follower_count' to be at least of size 1.000. Finally we then filtered our 'relevant_players' tibble by an inner join with this selection on their common variable 'screen_name'.

```{r select_relevant_players, eval = TRUE}
relevant_players <- player_season_stats %>% 
  
  # Players who played actively (>= 80% of games) for the same team between 2016-2019 (3 seasons)
  filter(Season %in% c("2016-17", "2017-18", "2018-19")) %>% 
  filter(SeasonType == "Regular Season") %>% 
  group_by(BBRef_Player_ID) %>% 
  filter(n() == 3) %>% 
  summarise(
    team_cnt = length(unique(Team)),
    game_cnt = sum(G)
  ) %>% 
  filter(team_cnt == 1) %>% 
  filter(game_cnt >= 0.8 * 82 * 3) %>% 
  select(BBRef_Player_ID) %>% 
  
  # Players whose BPM varied by a standard deviation of at least 8
  inner_join(player_game_stats) %>% 
  group_by(BBRef_Player_ID) %>% 
  summarise(sd_BPM = sd(BPM, na.rm = TRUE)) %>% 
  filter(sd_BPM >= 8) %>% 
  select(BBRef_Player_ID) %>% 
  
  # Players with at least 1000 followers
  inner_join(player_metadata) %>% 
  pmap_dfr(function(...){
    relevant_player <- tibble(...)
    twitter_meta <- relevant_player$Twitter %>% 
      lookup_users(token = twitter_token) %>% 
      select(c("screen_name", "followers_count"))
    inner_join(relevant_player, twitter_meta, by = c("Twitter" = "screen_name"))
  }) %>% filter(followers_count >= 1000) %>% 
  select(player_metadata %>% names())
```

#### 4.2.4 Merging the parameters
Finally we merged the cut off standard deviation values of the players with their respective twitter-account data, including the count of followers, the count of posted statuses, the count of accounts indicated as favourites and the player's screen name.Now the last step was to create a final set of players we wanted to consider in our analysis by merging the two data sets created into one and picking the top intersecting players.

```{r display_relevant_players, eval = TRUE, message = TRUE}
relevant_players
```


```{r select_relevant_player_stats, eval = TRUE}
relevant_player_stats <- relevant_players %>% 
  inner_join(player_game_stats) %>% 
  filter(! is.na(MP)) %>% 
  inner_join(game_metadata) %>% 
  filter(Season %in% c("2017-18", "2018-19"))
```


```{r display_relevant_player_stats, eval = TRUE, message = TRUE}
relevant_player_stats
```


### 4.3 Extracting the relevant tweets
With the given data we were now able to extract exactly those tweets we needed for our analysis. To do so, we chose to use the get_all_tweets function from the academictwitteR packagage.

To only extract tweets that can be assumed t
<!--
```{r extract_and_save_tweets}
time_window <- 48
tweet_cols <- c("BBRef_Player_ID", "BBRef_Game_ID", "id", "text", "created_at", "retweet_count", "reply_count", "like_count", "quote_count")
alrdy_proc_plyrs <- list.files(tweets_dir) %>% map_chr(~ str_remove(.x, "\\.csv$"))


relevant_player_stats %>% 
  filter(! Twitter %in% c(alrdy_proc_plyrs)) %>% 
  mutate(EndTweet = DateTime - lubridate::dminutes(45)) %>% 
  mutate(StartTweet = EndTweet - lubridate::dhours(time_window)) %>% 
  
  # Save tweets player-wise
  group_by(BBRef_Player_ID) %>% 
  group_walk(~ {
    tweets <- .x %>% pmap_dfr(function(...) {
      player_game_data <- tibble(...)
      
      start_tweets = player_game_data$StartTweet %>% iso8601() %>% paste0("Z")
      end_tweets = player_game_data$EndTweet %>% iso8601() %>% paste0("Z")
      
      twts <- tibble()
      
      tryCatch({
          twts <- get_all_tweets(
            query = player_game_data$Twitter,
            start_tweets = start_tweets,
            end_tweets = end_tweets,
            is_retweet = FALSE,
            lang = "en",
            bearer_token = twitter_bearer_token
          ) %>% 
            mutate(BBRef_Player_ID = player_game_data$BBRef_Player_ID) %>% 
            mutate(BBRef_Game_ID = player_game_data$BBRef_Game_ID)
        }, error = function(e) {
          # For Status Code 503
          print("Error loading tweets for the following game: ")
          print(player_game_data %>% select(c(Twitter, BBRef_Game_ID, DateTime, StartTweet, EndTweet)))
        }, finally = {
          return(twts)
        }
      )
      
    }) %>% 
      mutate(retweet_count = public_metrics$retweet_count) %>% 
      mutate(reply_count = public_metrics$reply_count) %>% 
      mutate(like_count = public_metrics$like_count) %>% 
      mutate(quote_count = public_metrics$quote_count) %>% 
      select(tweet_cols)
    
    file_name <- .x$Twitter %>% unique() %>% paste0(".csv")
    file_path <- tweets_dir %>% paste(file_name, sep = "/")
    
    tweets %>% write_csv(file_path)
  })
```
-->


```{r load_tweets_1, eval = TRUE}
tweets <- list.files(tweets_dir, full.names = TRUE) %>% 
  map_dfr(read_csv)

tweets <- player_metadata %>% inner_join(tweets)
```


```{r display_tweets, eval = TRUE, message = TRUE}
tweets
```


# 5. Preprocessing the Twitter Data
After we finished setting up our data sets and before extracting sentiments from the tweets it is reasonable to preprocess them in advance in order to improve the accuracy of the computed sentiments, especially for our case where the text is in the form of tweets. Tweets are most often written in a less formal language, including abbrevations and slang, so our focus layed in identifying and converting these phenomenons into a language that has increased machine readability. That is why we used the `textclean` package to apply the following preprocessing steps on each tweet: 

At first each tweet got lowercased. Then we resolved non-ascii characters, replaced html-symbols by word meanings (e.g. "&amp" to "and") so that they can be captured by the analyzer and replaced (multiple succeeding) white space symbols by single white spaces (e.g. "\\t" by " "). After that, Twitter mentions, hashtags as well as URLs (e.g. "@StephenCurry30", "#BBNFOREVER", "https://t.co/37cSfQhMJs") got removed as they give us no further information about the sentiments and only force the analyzer to run over more words to check in the lexicon. Additionally we replaced emojis by word meaning (e.g. ":)" to "smiley"). Similar to how we treated html-symbols, contractions got replaced by their multi-word forms (e.g. "I'll" to "I will") again for the sake of machine readability. 
We replaced common words, which letters were written with spaces in between (for emphasis), by their semantic equivalent without spaces (e.g. "B O M B" to "BOMB") and replaced word lengthenings to emphasize or alter word meanings by their semantic equivalent (e.g. "niiiice" to "nice"). Finally, internet slang and abbreviations got replaced by their semantic equivalent (e.g. "YOLO" to "you only live once").

A very important decision that should be noted is, that stemming (i.e. Porter Stemming) was not applied to the tweets since the terms are written in their base form in the sentiment lexicons. Additionally stopword-removal was not performed to avoid the risk of removing potentially crucial valence shifters for the sentiment extraction (e.g. in "I am **not** happy" the term "**not**" negates the sentiment and should therefore not be removed). These valence shifters will play a significant role in the sentiment analyzer we chose to work on our data and we will get back to this in a later section.

```{r load_tweets_2, eval = TRUE}
tweets <- list.files(tweets_dir, full.names = TRUE) %>% 
  map_dfr(read_csv) %>% 
  select(c(id, text)) %>% 
  
  # Eliminate duplicate tweets (addressed to multiple players) to accelerate processing
  unique() 
```

```{r preprocess_tweets}
prep_tweets <- tweets %>% 
  mutate(prep_text = imap_chr(tweets$text, ~{
    
    print(.y %>% paste(nrow(tweets), sep = "/"))
      
    # Replace words which letters are written with spaces in between for emphasis by their semantic equivalence (e.g. "B O M B" -> "BOMB")
    txt <- replace_kern(.x)
    
    # Replace emoji by description
    txt <- replace_emoji(txt)
    
    # Replace non-ascii characters by semantic equivalent
    txt <- replace_non_ascii(txt)
    
    # Replace contractions by their multi-word forms (e.g. "I'll" -> "I will")
    txt <- replace_contraction(txt)
    
    # Text to lowercase 
    txt <- str_to_lower(txt)
    
    # Remove Twitter URL's, mentions and hashtags
    txt <- replace_url(txt) 
    txt <- replace_tag(txt)
    txt <- replace_hash(txt)
    
    # Replace html symbols by semantic equivalent symbol (e.g. "&amp;" -> "and")
    txt <- replace_html(txt)
    txt <- replace_symbol(txt)
    
    # Replace word lenghenings to emphasize or alter word meanings by their semantic equivalence (e.g. "I said heyyy!" -> "I said hey sexy!")
    txt <- replace_word_elongation(txt, impart.meaning = TRUE) 
    
    # Replace internet slang by their semantic equivalence (e.g. "YOLO" -> "You only live once")
    txt <- replace_internet_slang(txt)
    
    # Replace white space characters by single white space and trim
    txt <- replace_white(txt)
    txt <- str_trim(txt)
    
  }))
  
```

One of the first issues we were confronted with when we started looking into our extracted twitter data and ran a first sentiment analysis on it, was that emojis didn't get processed properly by any analyzer we tested. But in our opinion, no component of a tweet carries emotions so strongly than these emojis (which of course already gets implied by the name). So, besides the preprocessing of the textual information of the tweets themselves, we decided to additionally handle emojis separately by letting a special emoji-analyzer, the **_Novak_** Emoji Sentiment Lexicon, run over them. They were extracted from each tweet and stored by their key representation (from the **_Novak_** Emoji Sentiment Lexicon) in a separate variable separated by white spaces in order to use them for an encapsulated emoji sentiment computation for the individual tweets.

```{r extract_emojis}
emoji_sentiment_lexicon <- lexicon::hash_sentiment_emojis %>% tibble()
emoji_regex <- emoji_sentiment_lexicon$x %>% paste(collapse = "|")

prep_tweets <- prep_tweets %>% 
  mutate(emojis = map_chr(tweets$text, ~ {
    .x %>% 
      replace_emoji_identifier() %>% 
      str_extract_all(emoji_regex) %>% 
      unlist() %>% 
      paste(collapse = " ")
  }))
```


```{r store_preprocessed_tweets}
file_path <- data_dir %>% paste("prep-tweets.csv", sep = "/")

prep_tweets %>% 
  select(c(id, prep_text, emojis)) %>% 
  write_csv(file_path)
```


```{r load_preprocessed_tweets, eval = TRUE}
prep_tweets <-data_dir %>% 
  paste("prep-tweets.csv", sep = "/") %>% 
  read_csv()
```

The following table gives an idea about how the tweets look like before and after the performed preprocessing steps: 

```{r display_preprocessed_tweets, eval = TRUE}
tweets %>%
  inner_join(prep_tweets) %>% 
  select(c(text, prep_text, emojis)) %>% 
  unique()
```

# 6. Sentiment Extraction
Our next task was to extract sentiments from the preprocessed tweets and extracted emojis. To solve this task we used the package `sentimentr`, since compared to other solutions (e.g. `syuzhet` and `tidytext`), `sentimentr` uses an orderd bag of words model that allows it to incorporate valance shifters before or after polarized words to negate or intensify their sentiment (e.g. "I do **not** like it!" or "I **really** like it!"). That ultimately gives `sentimentr` the power to much more accurately assign sentiments to text passages. 

The sentiments were computed sentence-wise for each tweet and aggregated via the ``average_downweighted_zero`` ``sentimentr``-function that downweights sentiment-scores for sentences close to zero. 

The following sentiment lexica were used to compute the sentiments for each tweet by making use of the `lexicon` package: 

* **_Bing_**: positive/negative word list created by Hu Xu and Bing Liu (TODO reference).
* **_Syuzhet_**: word list with sentiment scores reaching from -1 to 1 created by Matthew L. Jockers (TODO reference).
* **_Jockers-Rinker_**: combined version of the Jocker's **_Syuzhet_** lexicon and Rinker's augmented **_Bing_** lexicon (TODO reference).
* **_NRC_**: positive/negative word list created by Saif M. Mohammad (TODO reference).
* **_AFINN_**: word list with sentiments reaching on a discrete scale from -5 to 5 created by Finn Årup Nielsen (TODO reference).
* **_Novak_**: list of emojis with sentiment scores reaching from -1 to 1 created by Kralji Novak (TODO reference http://kt.ijs.si/data/Emoji_sentiment_ranking/index.html). It should be noted that this sentiment lexicon was only applied on the extracted emojis but not on the text of the preprocessed tweets. Furthermore tweets that contained no emojis were excluded from the sentiment computation. 

```{r load_tweets_3, eval = TRUE}
tweets <- list.files(tweets_dir, full.names = TRUE) %>% 
  map_dfr(read_csv)

prep_tweets <- data_dir %>% paste("prep-tweets.csv", sep = "/") %>% read_csv()

# tweets %>% inner_join(prep_tweets) %>% select(c(text, prep_text, emojis))
```


```{r bing_sentiments}
bing_sentiments <- prep_tweets %>% 
  mutate(sentences = get_sentences(prep_text)) %$%
  sentiment_by(sentences, list(id), polarity_dt = lexicon::hash_sentiment_huliu) %>% 
  tibble()

file_path <- sentiments_dir %>% paste("bing.csv", sep = "/")
bing_sentiments %>% write_csv(file_path)

bing_sentiments
```


```{r syuzhet_sentiments}
syuzhet_sentiments <- prep_tweets %>% 
  mutate(sentences = get_sentences(prep_text)) %$%
  sentiment_by(sentences, list(id), polarity_dt = lexicon::hash_sentiment_jockers) %>% 
  tibble()

file_path <- sentiments_dir %>% paste("syuzhet.csv", sep = "/")
syuzhet_sentiments %>% write_csv(file_path)

syuzhet_sentiments
```


```{r jockers_rinker_sentiments}
jockers_rinker_sentiments <- prep_tweets %>%
  mutate(sentences = get_sentences(prep_text)) %$%  
  sentiment_by(sentences, list(id)) %>% 
  tibble()

file_path <- sentiments_dir %>% paste("jockers-rinker.csv", sep = "/", polarity_dt = lexicon::hash_sentiment_jockers_rinker)
jockers_rinker_sentiments %>% write_csv(file_path)

jockers_rinker_sentiments
```


```{r nrc_sentients}
nrc_sentiments <- prep_tweets %>% 
  mutate(sentences = get_sentences(prep_text)) %$%
  sentiment_by(sentences, list(id), polarity_dt = lexicon::hash_sentiment_nrc) %>% 
  tibble()

file_path <- sentiments_dir %>% paste("nrc.csv", sep = "/")
nrc_sentiments %>% write_csv(file_path)

nrc_sentiments
```


```{r afinn_sentiments}
# Note: download for "afinn" has to be confirmed
hash_sentiment_afinn <- tidytext::get_sentiments("afinn") %>% 
  rename(c(x = word, y = value)) %>% 
  as_key() # convert tibble to data table for sentimentr

afinn_sentiments <- prep_tweets %>% 
  mutate(sentences = get_sentences(prep_text)) %$%
  sentiment_by(sentences, list(id), polarity_dt = hash_sentiment_afinn) %>% 
  tibble()

file_path <- sentiments_dir %>% paste("afinn.csv", sep = "/")
afinn_sentiments %>% write_csv(file_path)

afinn_sentiments
```


```{r novak_emoji_sentiments}
novak_emoji_sentiments <- prep_tweets %>%
  filter(emojis != "") %>% 
  filter(! is.na(emojis)) %>%
  mutate(emojis = replace_emoji_identifier(emojis)) %>% 
  mutate(sentences = get_sentences(emojis)) %$%
  sentiment_by(sentences, list(id), polarity_dt = lexicon::hash_sentiment_emojis) %>% 
  tibble() 

file_path <- sentiments_dir %>% paste("novak-emoji.csv", sep = "/")
novak_emoji_sentiments %>% write_csv(file_path)

novak_emoji_sentiments 
```
