---
title: "Predictive Model"
weight: 5
chapter: yes
---

<script src="/rmarkdown-libs/header-attrs/header-attrs.js"></script>


<style>#chapter p {
text-align: justify} #chapter h3 {
text-align: left}
</style>
<div id="predictive-model" class="section level2">
<h2>5. Predictive Model</h2>
<p>We setup a random forest regression model to predict the BPM of players. The dataset is the one previously used.
We set up different models:</p>
<ul>
<li>mean BPM of the last 5 BPMs as a baseline model</li>
<li>model including only the sentiment scores</li>
<li>model including the last 5 BPMs, Position, Age, Month of the Game, Homegame, Trend, SRS_Team, SRS_Opponent</li>
<li>model including the last 5 BPMs, Position, Age, Month of the Game, Homegame, Trend, SRS_Team, SRS_Opponent and the sentiment scores</li>
</ul>
<p>Homegame indicates if the player played a homegame (1) or an away game (0). The Trend indicates the teams last 5 game performances - i.e. the sum of wins (+1) and losses (-1) is calculated. In order to measure a more longterm team performance we use SRS (Simple Rating System) which gives a score to each team according to their average point difference and strength of schedule. 0 marks the average score. The SRS for the previous season is used for the prediction.</p>
<p>We start by selecting the relevant columns for the model and split the data into training and testing data. A validation split is performed on the training data in order to combat overfitting. Then each model is created using the recipe package. Using tune_grid() the model parameter mtry (number of predictors that will be randomly sampled at each split when creating the tree models), trees (number of trees contained in the ensemble) and min_n (minimum number of data points in a node that are required for the node to be split further) get tuned. The best model fit gets choosen based on the RMSE (Root Mean Square Error) which calculates the average distance between the predicted values and the actual values, i.e. the lower the RMSE, the better the model is able to fit a dataset. The importance score for each variable is saved as well.</p>
<p>In order to determine which model predicted the BPM best a visualization is generated that displayes the predicted value (y-axis) and the true value (x-axis) for each model, meaning a perfectly fitted model would be the 45° line. The visualization doesn’t allow for a clear interpretation since all models are scattered and no specific trend can be observed.</p>
<p>In order to determine which model predicted the values best, the RMSE and RSQ (R-Squared: proportion of variance in the dependent variable that can be explained by the independent variables) is used to further analyze the different models.
The bar plot shows that the RMSE are in the range of 7.7 - 8.15. Using RMSE as a metric the model without sentiment scores predicts the BPM best. The difference between the models RSQ on the other hand is quite large. All models don’t explain the BPM well ranging from 0.0186973 to 0.0186973. The models including sentiments scores best, directly followed by the model without sentiments.</p>
<p>Looking at the variable importance scores for the model including sentiments it can be observed that the syuzhet and jockers_rinker sentiment scores rank second and third. The (permutation) importance score is calculated by (1) measuring a basline RSQ , (2) permute the values of one feature and measure the RSQ. The importance score is the difference between the basline and the drop in overal RSQ caused by the permutation.
The plot indicates a high influence of sentiment scores. But since the model has a low RSQ, this should be perceived carefully.
To further understand the influence of sentiment scores, a new model should be deployed with the goal to increase the RSQ and run the feature importance analysis again.</p>
<p>[TODO] Grafiken einbauen</p>
</div>
