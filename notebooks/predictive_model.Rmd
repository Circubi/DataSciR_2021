---
title: "Predictive Model"
output: html_document
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(lubridate)
library(tidyverse)
library(purrr)
library(rvest)
library(ranger)
library(tidymodels)
```

One attempt to check whether the sentiments have an influence on the player's performance at all and therefore to investigate our findings in the correlation analysis, was to set up a random forest regression model to predict the BPM of players. The idea was to set up different models and compare the predictions of performance with and without the sentiments as input features:

 - mean BPM of the last 5 BPMs as a baseline model
 - model including only the sentiment scores
 - model including the last 5 BPMs, Position, Age, Month of the Game, Homegame, Trend, SRS_Team, SRS_Opponent
 - model including the last 5 BPMs, Position, Age, Month of the Game, Homegame, Trend, SRS_Team, SRS_Opponent and the sentiment scores
 
In detail, Homegame expresses whether the player played a homegame (1) or an away game (0). The Trend indicates the teams last 5 game performances - i.e. the sum of wins (+1) and losses (-1) is calculated. In order to measure a more longterm team performance we used the SRS (Simple Rating System) which gives a score to each team according to their average point difference and strength of schedule and where 0 marks the average score. The SRS for the previous season was used for the prediction.

We started by loading the tweets and extracted the necessary columns before transforming them. 
```{r load_tweets_and_transform, message = FALSE}
data_dir <- "../data"
sentiments_dir <- "../data/sentiments"
tweets_dir <- data_dir %>% paste("tweets", sep = "/")

tweets <- list.files(tweets_dir, full.names = TRUE) %>% 
  map_dfr(read_csv)

player_game_stats <- data_dir %>% paste("player-game-stats.csv", sep = "/") %>% read_csv()
player_season_stats <- data_dir %>% paste("player-season-stats.csv", sep = "/") %>% read_csv()

games <- tweets %>% 
  select(BBRef_Player_ID ,BBRef_Game_ID) %>%
  unique() %>%
  left_join(player_game_stats %>% select(BBRef_Player_ID,BBRef_Game_ID,Season, Date, Tm, HTm, Opp, WL,BPM), by = c("BBRef_Player_ID" = "BBRef_Player_ID", "BBRef_Game_ID" = "BBRef_Game_ID")) %>%
  left_join(player_season_stats %>% select(BBRef_Player_ID,Season, Age, Pos), by = c("BBRef_Player_ID" = "BBRef_Player_ID", "Season" = "Season")) %>%
  unique()

player_game_stats <- player_game_stats %>%
  mutate(index = 1:nrow(player_game_stats)) %>% # get an index in order to use for the last 5 BPMs
  mutate(WL_ = ifelse(str_detect(player_game_stats$WL, "W"),1,-1)) # use for trend variable

games <- games %>%
  mutate(Homegame = ifelse(games$Tm == games$HTm,1,0)) %>%
  mutate(Month = month(Date))

get_BPM_Trend <- function(player, game){
  index <- player_game_stats %>%
    filter(BBRef_Player_ID == player,BBRef_Game_ID == game) %>%
    select(index) %>%
    pull()
  
  games <- games %>%
    filter(BBRef_Player_ID == player,BBRef_Game_ID == game) %>%
    mutate(BPM_5 = player_game_stats$BPM[index-5]) %>%
    mutate(BPM_4 = player_game_stats$BPM[index-4]) %>%
    mutate(BPM_3 = player_game_stats$BPM[index-3]) %>%
    mutate(BPM_2 = player_game_stats$BPM[index-2]) %>%
    mutate(BPM_1 = player_game_stats$BPM[index-1]) %>%
    mutate(Trend = sum(player_game_stats$WL_[(index-5):(index-1)]))
}

games <- map2_dfr(games$BBRef_Player_ID, games$BBRef_Game_ID, ~ get_BPM_Trend(.x, .y)) # get the last 5 BPMs and Trend for each player/game combination
```

After that the SRS (Simple Rating System) for the previous season ot extracted. This was done by parsing the html table from [basketball-reference.com](https://basketball-reference.com) and using the teams shortcut to combine the data with the previous dataset.
```{r load_basketball_reference_data, message = FALSE}
content <- read_html("https://www.basketball-reference.com/leagues/NBA_2018.html")
tables <- content %>% html_table(fill = TRUE)
team_stats_2018 <- tables[[11]]

team_stats_2018 <- team_stats_2018[,c(2,10)] %>% 
  tail(-1) %>% 
  head(-1) %>%
  setNames(c("Team","SRS")) %>%
  mutate(SRS = as.double(SRS)) %>%
  mutate(Team_name = c("HOU","TOR","GSW","UTA","PHI","BOS","OKC","SAS","POR","MIN","DEN","IND","NOP","CLE","WAS","MIA","CHO","LAC","DET","MIL","LAL","DAL","NYK","BRK","ORL","ATL","MEM","CHI","SAC","PHO")) %>%
  mutate(Season_plus_one = "2018-19")

content <- read_html("https://www.basketball-reference.com/leagues/NBA_2017.html")
tables <- content %>% html_table(fill = TRUE)
team_stats_2017 <- tables[[11]]

team_stats_2017 <- team_stats_2017[,c(2,10)] %>% 
  tail(-1) %>% 
  head(-1) %>%
  setNames(c("Team","SRS")) %>%
  mutate(SRS = as.double(SRS)) %>%
  mutate(Team_name = c("GSW","SAS","HOU","TOR","LAC","UTA","CLE","BOS","WAS","MIA","OKC","MEM","DEN","CHI","CHO","IND","MIL","POR","ATL","DET","MIN","NOP","DAL","NYK","SAC","PHO","PHI","BRK","ORL","LAL")) %>%
  mutate(Season_plus_one = "2017-18")

team_stats <- rbind(team_stats_2017,team_stats_2018)

games <- games %>% 
  left_join(team_stats, by = c("Season" = "Season_plus_one", "Tm" = "Team_name")) %>%
  left_join(team_stats, by = c("Season" = "Season_plus_one", "Opp" = "Team_name"))
```

Then some of the variables got renamed and the mean BPM of the last 5 BPMs got calculated.
```{r rename_columns, message = FALSE}
data <- games %>%
  select(BBRef_Player_ID,BBRef_Game_ID,Age,Pos,BPM_5,BPM_4,BPM_3,BPM_2,BPM_1,Homegame, Month,SRS_Tm = SRS.x , SRS_Opp = SRS.y, BPM, Trend) %>%
  mutate(mean_BPM = rowMeans(games %>%select(BPM_5,BPM_4,BPM_3,BPM_2,BPM_1),na.rm = TRUE)) 
```

In the last pre-processing step, the already computed sentiments got merged with the newly created data.
```{r combine_sentiments_with_data, message = FALSE}
prep_tweets <- data_dir %>% paste("prep-tweets.csv", sep = "/") %>% read_csv()
bing <- sentiments_dir %>% 
  paste("bing.csv", sep = "/") %>% 
  read_csv() %>% 
  select(c(id, word_count, ave_sentiment)) %>% 
  rename(bing = ave_sentiment) %>%
  mutate(binary_sentiment = if_else(bing < 0.0, "negative", "positive"))

syuzhet <- sentiments_dir %>% 
  paste("syuzhet.csv", sep = "/") %>% 
  read_csv() %>% 
  select(c(id, ave_sentiment)) %>% 
  rename(syuzhet = ave_sentiment) %>%
  mutate(binary_sentiment = if_else(syuzhet < 0.0, "negative", "positive"))

jockers_rinker <- sentiments_dir %>% 
  paste("jockers-rinker.csv", sep = "/") %>% 
  read_csv() %>% 
  select(c(id, ave_sentiment)) %>% 
  rename(jockers_rinker = ave_sentiment) %>%
  mutate(binary_sentiment = if_else(jockers_rinker < 0.0, "negative", "positive"))

afinn <- sentiments_dir %>% 
  paste("afinn.csv", sep = "/") %>% 
  read_csv() %>% 
  select(c(id, ave_sentiment)) %>% 
  rename(afinn = ave_sentiment) %>%
  mutate(binary_sentiment = if_else(afinn < 0.0, "negative", "positive"))

nrc <- sentiments_dir %>% 
  paste("nrc.csv", sep = "/") %>% 
  read_csv() %>% 
  select(c(id, ave_sentiment)) %>% 
  rename(nrc = ave_sentiment) %>%
  mutate(binary_sentiment = if_else(nrc < 0.0, "negative", "positive"))

novak_emoji <- sentiments_dir %>% 
  paste("novak-emoji.csv", sep = "/") %>% 
  read_csv() %>% 
  select(c(id, ave_sentiment)) %>% 
  rename(novak_emoji = ave_sentiment) %>%
  mutate(binary_sentiment = if_else(novak_emoji < 0.0, "negative", "positive"))

sentiments <- tweets %>% 
  inner_join(prep_tweets) %>% 
  select(c(BBRef_Player_ID,BBRef_Game_ID,id, text, prep_text)) %>% 
  inner_join(bing) %>%
  inner_join(syuzhet) %>% 
  inner_join(jockers_rinker) %>% 
  inner_join(nrc) %>% 
  inner_join(afinn) %>% 
  inner_join(novak_emoji)

aggregated_sentiments <- sentiments %>% 
  group_by(BBRef_Player_ID, BBRef_Game_ID) %>% 
  summarize(bing = mean(bing), syuzhet = mean(syuzhet), jockers_rinker = mean(jockers_rinker), 
            nrc = mean(nrc), afinn = mean(afinn), novak_emoji = mean(novak_emoji))

data <- data %>% 
  left_join(aggregated_sentiments, by = c("BBRef_Player_ID" = "BBRef_Player_ID", "BBRef_Game_ID" = "BBRef_Game_ID"))


data <- data %>% drop_na(BPM,BPM_5,BPM_4,BPM_3,BPM_2,BPM_1,bing,syuzhet,jockers_rinker,nrc,afinn)
```

```{r plot_BPM, message = FALSE}
data %>%
  ggplot(aes(fct_reorder(BBRef_Player_ID,BPM), 
             BPM)) +
  geom_boxplot(outlier.alpha = 0.5) +
  coord_flip() +
  xlab("BPM performance") +
  ylab("BBRef_Player_ID")
```

For the actual prediction task we started by selecting the relevant columns for the model and split the data into training and testing data with a proportion of 75% training to 25% testing. A validation split of 20% of the training set was then performed data in order to combat overfitting. Then each model was created using the recipe package. Using tune_grid(), the model parameter mtry (number of predictors that will be randomly sampled at each split when creating the tree models), trees (number of trees contained in the ensemble) and min_n (minimum number of data points in a node that are required for the node to be split further) got tuned. After tuning, the best model fit got choosen based on the RMSE (Root Mean Square Error) which calculates the average distance between the predicted values and the actual values, i.e. the lower the RMSE, the better the model is able to fit a dataset. The importance score for each variable was saved as well.

```{r predictive_model, message = FALSE}
data <- data %>%
  select(Pos, Age, BPM_5, BPM_4, BPM_3, BPM_2, BPM_1, Homegame, Month, SRS_Tm, SRS_Opp, Trend, bing, syuzhet, jockers_rinker, nrc, afinn, novak_emoji, BPM, mean_BPM) %>% 
  drop_na(BPM,BPM_5,BPM_4,BPM_3,BPM_2,BPM_1,bing,syuzhet,jockers_rinker,nrc,afinn)

set.seed(123)
data_split <- initial_split(data)
data_train <- training(data_split)
data_test <- testing(data_split)
val_set <- validation_split(data_train, prop = 0.8)

# normal model
rf_rec <- recipe(BPM ~ BPM_5 + BPM_4 + BPM_3 + BPM_2 + BPM_1 + Pos + Age + Month + Homegame + Trend + SRS_Tm +  SRS_Opp, data = data_train) 

# model with sentiments
rf_rec_with_sentiment <- recipe(BPM ~ BPM_5 + BPM_4 + BPM_3 + BPM_2 + BPM_1 + Pos + Age + Month + Homegame + Trend + SRS_Tm +  SRS_Opp + bing + syuzhet + jockers_rinker + nrc + afinn + novak_emoji, data = data_train) 

# model with mean BPM
rf_rec_mean <- recipe(BPM ~ mean_BPM, data = data_train)

# model sentiments only
rf_rec_only_sentiment <- recipe(BPM ~ bing + syuzhet + jockers_rinker + nrc + afinn + novak_emoji, data = data_train)


get_model <- function(rec){
  rf_spec <- rand_forest(
  mtry = tune(),
  min_n = tune(),
  trees = tune()) %>%
  set_engine("ranger") %>%
  set_mode("regression")
  
  rf_wf <- workflow() %>%
  add_model(rf_spec) %>%
  add_recipe(rec)  

  rf_res <- rf_wf %>%
  tune_grid(val_set,
            grid = 100,
            control = control_grid(save_pred = TRUE))  
  
  rf_best <- rf_res %>%
  select_best(metric = "rmse")
  
  last_rf_spec <- rand_forest(mtry = rf_best$mtry[1], min_n = rf_best$min_n[1], trees = rf_best$trees[1]) %>%
  set_engine("ranger", importance ="permutation") %>%
  set_mode("regression")

last_rf_wf <- rf_wf %>%
  update_model(last_rf_spec)

last_rf_fit <- last_rf_wf %>%
  last_fit(data_split)
}

model <- get_model(rf_rec)
model_with_sentiment <- get_model(rf_rec_with_sentiment)
model_mean_BPM <- get_model(rf_rec_mean)
model_only_sentiment <- get_model(rf_rec_only_sentiment)
```

In order to determine which model predicted the BPM performance best, a visualization is shown below that displays the predicted value (y-axis) and the true value (x-axis) for each model, meaning a perfectly fitted model would have all predictions on the the 45° line. The visualization doesn't allow for a clear interpretation since all models are scattered and no specific trend can be observed.

```{r predictions, message = FALSE}
predictions <- model$.predictions[[1]] %>%
  mutate(model = "without sentiments") %>%
  bind_rows(model_with_sentiment$.predictions[[1]] %>%
              mutate(model = "with sentiments")) %>%
  bind_rows(model_mean_BPM$.predictions[[1]] %>%
              mutate(model = "mean BPM")) %>%
  bind_rows(model_only_sentiment$.predictions[[1]] %>%
              mutate(model = "sentiments only"))

predictions %>%
  ggplot(aes(BPM, .pred, color = model)) +
  geom_abline(lty = 2, color = "gray80", size = 1.5) +
  geom_point(alpha = 0.3) +
  facet_wrap(~ model) + 
  theme(legend.position="none") +
  xlab("BPM performance") +
  ylab("predicted performance")
```

```{r metrics, message = FALSE}
metrics <- model$.metrics[[1]] %>%
  mutate(model = "without sentiments") %>%
  bind_rows(model_with_sentiment$.metrics[[1]] %>%
              mutate(model = "with sentiments")) %>%
  bind_rows(model_mean_BPM$.metrics[[1]] %>%
              mutate(model = "mean BPM")) %>%
  bind_rows(model_only_sentiment$.metrics[[1]] %>%
              mutate(model = "sentiments only"))

metrics %>%
  select(.metric, .estimate, model)

metrics %>%
  select(.metric, .estimate, model) %>%
  ggplot(aes(x = .estimate, y = fct_reorder(model,.estimate))) +
  geom_bar(stat = "identity", position = "dodge") +
  coord_flip() +
  facet_wrap(~ .metric, scales = "free") +
  geom_text(aes(label=round(.estimate,4)), position=position_dodge(width=0.9), vjust=-0.25) + 
  theme(legend.position="none") +
  theme(axis.text.x = element_text(angle=90, vjust=.5, hjust=1)) + 
  theme(axis.title.x = element_blank(), axis.title.y = element_blank())
```

Therefore, to determine which model predicted the values best, the RMSE and RSQ (R-Squared: proportion of variance in the dependent variable that can be explained by the independent variables) was used to further analyze the different models.
The bar plot shows that the RMSE are in the range of `r metrics %>% filter(.metric == "rmse") %>% select(.estimate) %>% min() %>% round(2)` - `r metrics %>% filter(.metric == "rmse") %>% select(.estimate) %>% max() %>% round(2)`. Using RMSE as a metric the model without sentiment scores predicts the BPM best. The difference between the models's RSQ on the other hand is quite large. Here, all models don't explain the BPM well ranging from `r metrics %>% filter(.metric == "rsq") %>% select(.estimate) %>% min()` to `r metrics %>% filter(.metric == "rsq") %>% select(.estimate) %>% min()`. The models including sentiments predict the BPM performance best, directly followed by the model without sentiments.
So even here, a direct influence of the sentiments as a feature can not be derived from the model, underlining our findings in the correlation analysis that the sentiments have no significant influence on a player's in-game performance.

```{r variable_importance, message = FALSE}
get_variable_importance <- function(x){
  list <- x %>%
  pluck(".workflow", 1) %>%
  pull_workflow_fit()
  
  variable <- list$fit$variable.importance %>% names()
  value <- list$fit$variable.importance %>% unlist() %>% unname()

  data.frame(variable, value) %>%
    mutate(sentiment = ifelse(variable %in% c("bing", "syuzhet", "jockers_rinker", "nrc", "afinn", "novak_emoji"),T,F)) %>%
    ggplot(aes(x = fct_reorder(variable,value), y = value, fill = sentiment)) +
    geom_bar(stat= "identity") +
    coord_flip() +
    theme(legend.position="none") +
    xlab("feature") +
    ylab("importance for prediction") +
    scale_fill_manual(values = c("#595959", "#DB2C2C")) 
} 

get_variable_importance(model)
get_variable_importance(model_with_sentiment)
get_variable_importance(model_only_sentiment)
```

A last step we did was to look at the importance scores of each sentiment analyzer method for the model including sentiments. Here it can be observed that the syuzhet and jockers_rinker sentiment scores rank first and second. The (permutation) importance score is calculated by (1) measuring a basline RSQ , (2) permuting the values of one feature and measure the RSQ. The importance score is the difference between the basline and the drop in overall RSQ caused by the permutation.
The plot indicates a high influence of sentiment scores, but since the model has a low RSQ, this should be perceived carefully. To further understand the influence of sentiment scores, a new model should be deployed in further research with the goal to increase the RSQ. 
