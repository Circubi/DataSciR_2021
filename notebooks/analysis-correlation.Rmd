---
title: "Exploratory Data Analysis (EDA)"
author: "Frank Dreyer"
date: "19 6 2021"
output: html_document
---

```{r setup, include=FALSE}
library(tidyverse)

knitr::opts_chunk$set(
  echo = FALSE, 
  eval = FALSE, 
  message = FALSE,
  warning = FALSE
)

data_dir <- "../data"
tweets_dir <- data_dir %>% paste("tweets", sep = "/")
sentiments_dir <- data_dir %>% paste("sentiments", sep = "/")
```


```{r load_nba_stats, eval = TRUE}
player_metadata <- paste(data_dir,"player-metadata.csv", sep ="/") %>% read_csv()
player_game_stats <- paste(data_dir,"player-game-stats.csv", sep ="/") %>% read_csv()
game_metadata <- paste(data_dir,"game-metadata.csv", sep ="/") %>% read_csv()
player_season_stats <- paste(data_dir,"player-season-stats.csv", sep ="/") %>% read_csv()
```


```{r load_tweets, eval = TRUE}
tweets <- list.files(tweets_dir, full.names = TRUE) %>% 
  map_dfr(read_csv)

prep_tweets <- data_dir %>% 
  paste("prep-tweets.csv", sep = "/") %>% 
  read_csv()
```


```{r load_sentiments, eval = TRUE}
sentiment_names <- c("bing", "syuzhet", "jockers_rinker", "afinn", "nrc", "novak_emoji")

sentiments <- sentiment_names %>% 
  map_dfr(~ {
    sentiment_name <- .x
    
    sentiment_file <- sentiments_dir %>% 
      paste(sentiment_name, sep = "/") %>% 
      paste0(".csv") %>% 
      str_replace("_", "-")
    
    sentiments_tmp <- read_csv(sentiment_file) %>% 
      mutate(positive_sentiment = if_else(ave_sentiment >= 0, TRUE, FALSE)) %>% 
      mutate(sentiment_lexicon = sentiment_name)
      
  })
```

## Computing Sentiment Aggregates

Since the sentiment scores were computed on a per-tweet basis we first had to aggregate the sentiment scores accordingly in order to capture the overall social media vibe players were receiving before games in a single number. For that purpose we considered  the sentiment scores of all tweets a respective player received in a 24-hour window before a respective game and aggregated them as follows:  

* The average of the sentiment scores (mean). 
* The average of the sentiment scores weighted by the retweet count of the associated tweet (weighted mean). 
* The proportion of tweets with a negative associated sentiment score (< 0).


```{r compute_sentiment_aggregates_24h_before_games, eval = TRUE}
tweets_24h_before_games <- player_game_stats %>%
  inner_join(game_metadata) %>% 
  inner_join(tweets) %>% 
  mutate(h_timediff_game = as.double(DateTime - created_at, units = "hours")) %>% 
  select(c(names(tweets), h_timediff_game))  %>%
  filter(h_timediff_game <= 24)

sentiment_aggregates_24h_before_games <- tweets_24h_before_games %>% 
  inner_join(sentiments) %>% 
  group_by(BBRef_Player_ID, BBRef_Game_ID, sentiment_lexicon) %>% 
  summarise(
    avg_sentiment = mean(ave_sentiment),
    avg_sentiment_retweet_cnt_weighted = weighted.mean(ave_sentiment, retweet_count),
    rel_freq_negative = sum(!positive_sentiment) / n()
  ) 
```

The following table shows an excerpt of the per-game computed sentiment aggregates for the different sentiment lexica: 

```{r display_sentiment_aggregates_24h_before_games, eval = TRUE, message = TRUE}
sentiment_aggregates_24h_before_games
```

## Univariate Distribution Analysis

At this point we had all the necessary data to analyze the association between the aggregated sentiment scores of tweets the players received before games and their performance within the games. 

Before analyzing bivariate distributions and potential correlations however we first wanted to get a general idea how the individual variables are distributed. 

Plotting the density curves for the unweighted average sentiment scores for the different sentiment lexicons and players revealed the following picture: 

```{r plot_density_curves_avg_sentiments, eval = TRUE, message = TRUE, fig.width = 10, fig.height = 30}
player_metadata %>% 
  inner_join(sentiment_aggregates_24h_before_games) %>% 
  ggplot(mapping = aes(x = avg_sentiment)) +
    geom_density() + 
    facet_grid(Player ~ sentiment_lexicon, scales = "free")
```

Looking at the individual density curves we observed that the distributions of the average sentiment scores rougly fit the bell curve of a Normal distribution despite a few exceptions (esp. for the averaged sentiments for the emoji sentiment lexicon by Novak). 

To check our normality assumption we also constructed QQ-plots for the unweighted average sentiment scores: 

```{r plot_qq_plot_avg_sentiments, eval = TRUE, message = TRUE, fig.width = 10, fig.height = 30}
player_metadata %>% 
  inner_join(sentiment_aggregates_24h_before_games) %>% 
  ggplot(mapping = aes(sample = avg_sentiment)) + 
    geom_qq(alpha = 0.2) + 
    geom_qq_line() + 
    facet_grid(Player ~ sentiment_lexicon, scales = "free")
```

The QQ-plots confirmed our assumption of normality, since despite some curve offs at the extremities (some observed extremes were more extreme than expected), most of the observed quantiles matched the expected quantiles of the fitted Normal distribution. 

A similar picture could be observed for the average weighted sentiment scores (weighted by their associated retweet count) as the following equivalent grid of QQ-plots shows:

```{r plot_qq_plot_avg_weighted_sentiments, eval = TRUE, message = TRUE, fig.width = 10, fig.height = 30}
player_metadata %>% 
  inner_join(sentiment_aggregates_24h_before_games) %>% 
  ggplot(mapping = aes(sample = avg_sentiment_retweet_cnt_weighted)) + 
    geom_qq(alpha = 0.2) + 
    geom_qq_line() + 
    facet_grid(Player ~ sentiment_lexicon, scales = "free")
```

The distributions for the negative tweet proportions mostly did not follow a Normal distribution and were strongly right skewed however. That intuitively made sense since in most of the cases players only receive a small proportion of tweets with a negative sentiment which leads to the right skewness of the distribution (also because proportions cannot go below 0). The following grid of density plots emphasize that circumstance:

```{r plot_density_curves_weighted_rel_freq_negative, eval = TRUE, message = TRUE, fig.width = 10, fig.height = 30}
player_metadata %>% 
  inner_join(sentiment_aggregates_24h_before_games) %>% 
  ggplot(mapping = aes(x = rel_freq_negative)) +
    geom_density() + 
    facet_grid(Player ~ sentiment_lexicon, scales = "free")
```

Besides the sentiment aggregates we also studied how the BPM perfomance indicator values are distributed for the different players. Similar to the unweighted and weighted sentiment averages before BPM was also normally distributed as the following grid of qq-plots indicates: 

```{r plot_qq_plot_BPM, eval = TRUE, message = TRUE, fig.width = 10, fig.height = 30}
player_metadata %>% 
  inner_join(player_game_stats) %>% 
  inner_join(sentiment_aggregates_24h_before_games) %>% 
  mutate(variable = "BPM") %>% 
  ggplot(mapping = aes(sample = BPM)) + 
    geom_qq(alpha = 0.2) + 
    geom_qq_line() + 
    facet_grid(Player ~ variable, scales = "free")
  
```

## Bivariate Distribution and Correlation Analysis

For this purpose we first plotted the unweighted average of the sentiment scores against the associated BPM values in a scatterplot for each player-sentiment lexicon combination and fitted a simple linear regression through each of the resulting point clouds. 

```{r avg_sentiments_BPM_plotting, eval = FALSE, message = TRUE, fig.width = 10, fig.height = 30}
player_metadata %>% 
  inner_join(player_game_stats) %>% 
  inner_join(sentiment_aggregates_24h_before_games) %>% 
  ggplot(mapping = aes(x = avg_sentiment_retweet_cnt_weighted, y = BPM)) + 
    geom_point(alpha = 0.2) + 
    geom_smooth(method = "lm", se = FALSE, color = "red") + 
    facet_grid(Player ~ sentiment_lexicon, scales = "free")
```

By inspecting the plots more closely we can see that none of the plots reveal a clear association between the two variables. 

To confirm our assumption with numbers we additionally wanted to 

To confirm our assumption with numbers we also aimed to compute the strength of the correlation by calculating the Pearson correlation coefficient between the aggregated sentiment score and the BPM performance indicator for each of the player - sentiment lexicon combinations. To apply the Pearson correlation coefficient we first had to check that both variables are normally distributed. To check for normality we first 


```{r compute_pearson_cor, eval = FALSE, message = TRUE}
sentiment_aggregates_24h_before_games %>% 
  pivot_wider(
    id_cols = c(BBRef_Player_ID, BBRef_Game_ID), 
    names_from = sentiment_lexicon, 
    values_from = avg_sentiment
  ) %>% 
  inner_join(player_game_stats) %>% 
  inner_join(player_metadata) %>% 
  group_by(Player) %>% 
  group_modify(~ {
    tibble(
      corr_jockers_rinker_BPM = cor(.x$jockers_rinker, .x$BPM, use = "complete.obs"),
      corr_nrc_BPM = cor(.x$nrc, .x$BPM, use = "complete.obs"),
      corr_afinn_BPM = cor(.x$afinn, .x$BPM, use = "complete.obs"),
      corr_novak_emoji_BPM = cor(.x$novak_emoji, .x$BPM, use = "complete.obs")
    )
  })
```


```{r compute_sentiment_consensus_binary, eval = FALSE}
sentiment_lexicons <- sentiments$sentiment_lexicon %>% unique()

# Create cross product of sentiment_lexicon with itself
sentiment_lexicons_a <- sentiment_lexicons %>% 
  rep(each = length(sentiment_lexicons))
sentiment_lexicons_b <- sentiment_lexicons %>% 
  rep(times = length(sentiment_lexicons))

sentiment_lexicon_consensus <- map2_dfr(sentiment_lexicons_a, sentiment_lexicons_b, ~ {
  
  sentiments_a <- sentiments %>% 
    filter(sentiment_lexicon == .x) %>% 
    select(id, positive_sentiment) %>% 
    filter(! is.na(positive_sentiment)) %>% 
    rename(positive_lexicon_a = positive_sentiment)
  
  sentiments_b <- sentiments %>% 
    filter(sentiment_lexicon == .y) %>% 
    select(id, positive_sentiment) %>% 
    filter(! is.na(positive_sentiment)) %>% 
    rename(positive_lexicon_b = positive_sentiment)
  
  inner_join(sentiments_a, sentiments_b) %>% 
    mutate(sentiment_lexicon_a = .x) %>% 
    mutate(sentiment_lexicon_b = .y) %>% 
    mutate(consensus = positive_lexicon_a == positive_lexicon_b) %>% 
    group_by(sentiment_lexicon_a, sentiment_lexicon_b) %>% 
    summarise(consensus = sum(consensus) / n()) %>% 
    ungroup()
  
})
```


```{r compute_sentiment_consensus_continuous, eval = FALSE}
sentiment_lexicons <- sentiments$sentiment_lexicon %>% unique()

# Create cross product of sentiment_lexicon with itself
sentiment_lexicons_a <- sentiment_lexicons %>% 
  rep(each = length(sentiment_lexicons))
sentiment_lexicons_b <- sentiment_lexicons %>% 
  rep(times = length(sentiment_lexicons))

sentiment_lexicon_consensus <- map2_dfr(sentiment_lexicons_a, sentiment_lexicons_b, ~ {
  
  sentiments_a <- sentiments %>% 
    filter(sentiment_lexicon == .x) %>% 
    mutate(standardized_sentiment = scale(ave_sentiment, center = TRUE, scale = TRUE)) %>% 
    mutate(pnorm_sentiment_a = pnorm(standardized_sentiment)) %>% 
    select(id, pnorm_sentiment_a) 
  
  sentiments_b <- sentiments %>% 
    filter(sentiment_lexicon == .y) %>%
    mutate(standardized_sentiment = scale(ave_sentiment, center = TRUE, scale = TRUE)) %>% 
    mutate(pnorm_sentiment_b = pnorm(standardized_sentiment)) %>% 
    select(id, pnorm_sentiment_b) 
  
  inner_join(sentiments_a, sentiments_b) %>% 
    mutate(sentiment_lexicon_a = .x) %>% 
    mutate(sentiment_lexicon_b = .y) %>% 
    mutate(pnorm_sentiment_abs_diff = abs(pnorm_sentiment_a - pnorm_sentiment_b)) %>% 
    group_by(sentiment_lexicon_a, sentiment_lexicon_b) %>% 
    summarise(consensus = 1 - mean(pnorm_sentiment_abs_diff)) %>% 
    ungroup()
  
})
```


```{r plot_sentiment_consensus, eval = FALSE, message = FALSE}
sentiment_lexicon_consensus %>% 
  ggplot(mapping = aes(x = sentiment_lexicon_a, y = sentiment_lexicon_b, fill = consensus)) +
    geom_tile() +
    colorspace::scale_fill_continuous_sequential("Purples") +
    theme_minimal()
```



Observation: On extremely positive sentiment values (outliers) the player had birthday. 

```{r birthday_outliers, eval = TRUE, message = FALSE}
sentiment_aggregates_24h_before_games %>% 
  filter(sentiment_lexicon == "nrc") %>% 
  group_by(BBRef_Player_ID) %>% 
  top_n(1, avg_sentiment) %>% 
  select(c(BBRef_Player_ID, BBRef_Game_ID)) %>% 
  inner_join(player_metadata) %>% 
  inner_join(tweets) %>% 
  filter(Player == "Draymond Green") %>% 
  select(c(Player, created_at, text))
```


```{r season_18_19_trend_BPM_vs_jockers_rinker, eval = FALSE, message = TRUE, fig.width = 10, fig.height = 80}
scaling_coef <- 60

player_metadata %>% 
  inner_join(player_game_stats) %>% 
  inner_join(sentiment_aggregates_24h_before_games) %>% 
  filter(SeasonType == "Regular Season") %>% 
  filter(Season %in% c("2018-19")) %>% 
  filter(sentiment_lexicon == "jockers_rinker") %>% 
  rename(jockers_rinker = avg_sentiment) %>% 
  ggplot(mapping = aes(x = Date)) +
  
    geom_smooth(mapping = aes(y = BPM), se = FALSE, color = "blue") + 
    geom_line(mapping = aes(y = BPM), color = "blue", alpha = 0.2) + 
  
    geom_smooth(mapping = aes(y = jockers_rinker * scaling_coef), se = FALSE, color = "red") +
    geom_line(mapping = aes(y = jockers_rinker * scaling_coef), color = "red", alpha = 0.2) + 
  
    facet_wrap(~ Player, ncol = 1) +
  
    scale_y_continuous( 
      name = "BPM",
      sec.axis = sec_axis(~./scaling_coef, name = "Jockers/Rinker")
    ) + 
    coord_cartesian(ylim = c(-10,20)) + 
    theme_minimal() + 
    theme(
      axis.title.y = element_text(color = "blue"),
      axis.title.y.right = element_text(color = "red")
    )
```

